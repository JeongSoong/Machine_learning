{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO94P3xYJf4MO1+ITVpCynD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongSoong/Machine_learning/blob/main/%EB%AC%B8%EC%A0%95%EC%88%98_201911047_14%EC%A3%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zrK2RwTZ-9A"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 300\n",
        "\n",
        "def preprocess(image, label):\n",
        "    image1 = tf.image.resize(image, size=(300, 300))\n",
        "    image2 = image1/255\n",
        "    return image2, label"
      ],
      "metadata": {
        "id": "0bN4JTIvbG3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solution_model():\n",
        "    (ds_train, ds_validation), ds_info = tfds.load(\n",
        "        name='beans',\n",
        "        split=['train', 'validation'],\n",
        "        as_supervised=True,\n",
        "        with_info=True\n",
        "    )\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    ds_train = ds_train.map(preprocess).cache().shuffle(\n",
        "        ds_info.splits['train'].num_examples).batch(BATCH_SIZE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    ds_validation = ds_validation.map(preprocess).batch(BATCH_SIZE).cache(\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        # 2D -> 1D로 변환을 위해 Flatten을 사용합니다.\n",
        "        Flatten(),\n",
        "        # 과적합 방지를 위해 Dropout을 적용합니다. 학습 시에 큰 노드 50%만 사용\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(3, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc']\n",
        "    )\n",
        "    epochs = 10\n",
        "    model.fit(\n",
        "        ds_train,\n",
        "        validation_data=(ds_validation),\n",
        "        epochs=epochs\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel_3.h5\")"
      ],
      "metadata": {
        "id": "aTBLeR62cCrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "datasets, info = tfds.load(name = 'beans', with_info=True,\n",
        "                           as_supervised = True,\n",
        "                           split = ['train', 'test', 'validation'])"
      ],
      "metadata": {
        "id": "mvUt_bF1gOXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, info_train = tfds.load(name = 'beans',\n",
        "                              with_info = True, split = 'test'\n",
        "                              )"
      ],
      "metadata": {
        "id": "Dj5NeSY0mGb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfds.show_examples(train, info_train)"
      ],
      "metadata": {
        "id": "7C5fP1qmmPQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3번유형(인공위성에서 촬영한 허리케인 분류)"
      ],
      "metadata": {
        "id": "y9c01sM3nZ7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "V6Ab_PgbnX-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/satellitehurricaneimages.zip'\n",
        "urllib.request.urlretrieve(url, 'satellitehurricaneimages.zip')\n",
        "\n",
        "with zipfile.ZipFile('satellitehurricaneimages.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "id": "jn8WYdHjn6Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yLqAHHVTtWdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_gen = ImageDataGenerator(rescale=1/255.,\n",
        "                               horizontal_flip=True,\n",
        "                               rotation_range=35,\n",
        "                               zoom_range=0.2\n",
        "                               )\n",
        "train_dir = '/content/train'\n",
        "train_gen = image_gen.flow_from_directory(train_dir,\n",
        "                                        target_size=(300, 300),\n",
        "                                        batch_size=128,\n",
        "                                        class_mode='binary'\n",
        "                                        )"
      ],
      "metadata": {
        "id": "lGQpxJb2tcBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = ['damage', 'no_damage']\n",
        "batch=next(train_gen)\n",
        "images, labels = batch[0], batch[1]\n",
        "\n",
        "plt.figure(figsize=(16,8))\n",
        "for i in range(32):\n",
        "\n",
        "    ax = plt.subplot(4,8,i+1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(class_labels[labels[i].astype(np.int)])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ivCMnbF4yFVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series Question(시계열 데이터, LSTM)"
      ],
      "metadata": {
        "id": "wyDoMrS3zOg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "# DO NOT CHANGE THIS.\n",
        "def windowed_dataset(series, batch_size, n_past=10, n_future=10, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Reads the dataset.\n",
        "    df = pd.read_csv('Weekly_U.S.Diesel_Retail_Prices.csv',\n",
        "                     infer_datetime_format=True, index_col='Week of', header=0)\n",
        "    \n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "    \n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    \n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.8) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "    N_PAST = 10  # DO NOT CHANGE THIS\n",
        "    N_FUTURE = 10  # DO NOT CHANGE THIS\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=32, kernel_size=5, padding='causal', activation='relu', input_shape=[N_PAST, 1]),\n",
        "        tf.keras.layers.Bidirectional(LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(LSTM(32, return_sequences=True)),        \n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),                  \n",
        "        tf.keras.layers.Dense(N_FEATURES) # 출력층\n",
        "    ])\n",
        "    \n",
        "    # Code to train and compile the model\n",
        "    checkpoint_path='model/my_checkpoint.ckpt'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             )\n",
        "    # Code to train and compile the model\n",
        "    optimizer =  tf.keras.optimizers.Adam(0.00003)  # YOUR CODE HERE\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
        "\n",
        "    model.fit(train_set, validation_data=(valid_set), epochs=150, callbacks=[checkpoint])\n",
        "\n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "eQFuDkJn1DfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NeYJf7nd92vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_zip=\"/content/HouseholdElectricPowerConsumption.zip\"\n",
        "zip_ref = zipfile.Zipfile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "PwJR7KMe96eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " df = pd.read_csv(\"/content/household_power_consumption.txt\", sep=',',\n",
        "                  parse_dates={'dt': ['Date', 'Time']},\n",
        "                     infer_datetime_format=True,\n",
        "                     low_memory=False, na_values=['nan','?'],\n",
        "                     index_col='dt')\n",
        " \n",
        " df.head()"
      ],
      "metadata": {
        "id": "dJiNpWtK-gLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the un windowed time series dataset to a\n",
        "# windowed dataset so as to prepare it for training and validation.\n",
        "# A window of features are constructed by shifting the window's starting\n",
        "# position forward, one at a time (indicated by shift=1).\n",
        "# For a window of 'n_past' number of observations of all time indexed variables in\n",
        "# the dataset, the target for the window is the next 'n_future' number\n",
        "# of observations of these variables, after the end of the window.\n",
        "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    # This line converts the dataset into a windowed dataset where a\n",
        "    # window consists of both the observations to be included as features\n",
        "    # and the targets.\n",
        "    # Don't change the shift parameter. The test windows are\n",
        "    # created with the specified shift and hence it might affect your\n",
        "    # scores. Calculate the window size so that based on\n",
        "    # the past 24 observations\n",
        "    # (observations at time steps t=1,t=2,...t=24) of the 7 variables\n",
        "    # in the dataset, you predict the next 24 observations\n",
        "    # (observations at time steps t=25,t=26....t=48) of the 7 variables\n",
        "    # of the dataset.\n",
        "    # Hint: Each window should include both the past observations and\n",
        "    # the future observations which are to be predicted. Calculate the\n",
        "    # window size based on n_past and n_future.\n",
        "    ds = ds.window(size=  # YOUR CODE HERE,\n",
        "                   shift = shift,\n",
        "                           drop_remainder = True)\n",
        "    # This line converts the windowed dataset into a tensorflow dataset.\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    # Now each window in the dataset has n_past and n_future observations.\n",
        "    # This line maps each window to the form (n_past observations,\n",
        "    # n_future observations) in the format needed for training the model.\n",
        "    # Note: You can use a lambda function to map each window in the\n",
        "    # dataset to it's respective (features, targets).\n",
        "    ds = ds.map(\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# This function loads the data from csv file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses the\n",
        "# 'windowed_dataset' to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network.\n",
        "# This function returns the trained model.\n",
        "#\n",
        "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "    # Reads the dataset from the csv.\n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
        "                     infer_datetime_format=True, index_col='datetime', header=0)\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features at future time steps.\n",
        "    N_FEATURES = len(df.columns)\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    split_time = int(len(data) * 0.5)\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "    # Splits the data into training and validation sets.\n",
        "    x_train = data[:split_time]\n",
        "    x_valid = data[split_time:]\n",
        "    # DO NOT CHANGE 'BATCH_SIZE' IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 24  # DO NOT CHANGE THIS\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 24  # DO NOT CHANGE THIS\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    # Complete the code in windowed_dataset.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # Whatever your first layer is, the input shape will be\n",
        "        # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n",
        "        # The model must have an output shape of\n",
        "        # (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n",
        "        # Make sure that there are N_FEATURES = 7 neurons in the final dense\n",
        "        # layer since the model predicts 7 features.\n",
        "        \n",
        "        Conv1D(filters=32,     kernel_size=3,       padding=\"causal\",\n",
        "            activation=\"relu\",       input_shape=[N_PAST, 7],\n",
        "            ),\n",
        "         Bidirectional(LSTM(32, return_sequences=True)),\n",
        "         Bidirectional(LSTM(32, return_sequences=True)),\n",
        "         Dense(32, activation=\"relu\"),\n",
        "         Dense(16, activation=\"relu\"),\n",
        "         Dense(N_FEATURES)\n",
        "\n",
        "        tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "    # Code to train and compile the model\n",
        "    optimizer =  # YOUR CODE HERE\n",
        "    model.compile(\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "    model.fit(\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "VHsskyHr9WKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "YxbgMaeC9kIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##데이터 확인하기\n",
        "\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "lr0v9cA5z5-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.dropbox.com/s/eduk281didil1km/Weekly_U.S.Diesel_Retail_Prices.csv?dl=1\"\n",
        "urllib.request.urlretrieve(url, 'Weekly_U.S.Diesel_Retail_Prices.csv')"
      ],
      "metadata": {
        "id": "1Um-TRS00CiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"Weekly_U.S.Diesel_Retail_Prices.csv\", infer_datetime_format=True,\n",
        "            index_col='Week of', header=0)"
      ],
      "metadata": {
        "id": "OAT9l5Pe0kqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}